# -*- coding: utf-8 -*-
"""DSCI 550 Final Project Code.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-HTuYk9U5NJCjnzG-JTwlsaWGEYLBsuI

#Load Library,Import Data, Descriptive Analysis
"""

import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from imblearn.under_sampling import RandomUnderSampler 
from sklearn import metrics

cancer = pd.read_csv('Breast_Cancer.csv')

cancer = cancer.drop('Survival_Months',axis=1)
cancer.head()

"""#Basic STAT"""

sns.boxplot(x = cancer['Status'] ,y = cancer['Reginol Node Positive']).set(title='Performance of Reginol Node Positive')

sns.boxplot(x = cancer['Status'] ,y = cancer['Tumor Size']).set(title='Tumor Size')

sns.displot(cancer, x = '6th Stage', hue = 'Status').set(title = '6th_Stage')

sns.displot(cancer, x = 'Estrogen Status', hue = 'Status').set(title = 'Estrogen Status')

sns.displot(cancer, x = 'N Stage', hue = 'Status').set(title = 'N_Stage')

sns.displot(cancer, x = 'Progesterone Status', hue = 'Status').set(title = 'Progesterone Status')

"""# Sampling"""

cancer = pd.get_dummies(cancer, drop_first = True)

cancer.head()

"""## Stratify Sampling"""

X = cancer.drop('Status_Dead', axis = 1)
y = cancer['Status_Dead']
X_try, X_ultimate, y_try, y_ultimate = train_test_split(X, y, test_size=0.15, stratify=y, random_state=36)

"""## Downsampling"""

rus = RandomUnderSampler(random_state=40)
X_res, y_res = rus.fit_resample(X_try, y_try)
X_train, X_test, y_train, y_test = train_test_split(X_res,y_res, test_size = 0.2, random_state = 20)

"""# Modeling

## Logistic Regression
"""

from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
import seaborn as sns
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from scipy.optimize import minimize

df = pd.read_csv('Breast_Cancer.csv')

df.columns = ['Age', 'Race', 'Marital Status', 'T Stage', 'N Stage', '6th Stage',
              'differentiate', 'Grade', 'A Stage', 'Tumor Size', 'Estrogen Status',
              'Progesterone Status', 'Regional Node Examined',
              'Regional Node Positive', 'Status']

df.loc[df['Grade']==' anaplastic; Grade IV', 'Grade'] = '4'
df.loc[df['Marital Status'] == 'Divorced', 'Marital Status'] = 'Other'
df.loc[df['Marital Status'] == 'Widowed', 'Marital Status'] = 'Other'
df.loc[df['Marital Status'] == 'Separated', 'Marital Status'] = 'Other'

data1 = pd.get_dummies(df)
data1 = data1.drop(columns = ['Status_Alive','Progesterone Status_Positive','Estrogen Status_Positive', 'A Stage_Regional'],axis = 1)

X = data1.drop('Status_Dead', axis = 1)
y = data1['Status_Dead']
X_try, X_ultimate, y_try, y_ultimate = train_test_split(X, y, test_size=0.15, stratify=y,random_state = 36)

"""### correlation in dataset"""

corr = X_try.corr() 

kot = corr[corr>=.9] 

plt.figure(figsize=(18,10)) 
plt.title('Correlation Matrix of the Cancer features',fontsize=15)

sns.heatmap(kot, cmap="Greens")

col = ['Age', 'Tumor Size', 'Regional Node Examined', 'Regional Node Positive',
       'Race_Black', 'Race_Other', 'Race_White', 'Marital Status_Married',
       'Marital Status_Other', 'Marital Status_Single ', 'T Stage_T1',
       'T Stage_T2', 'T Stage_T3', 'T Stage_T4', 'N Stage_N1', 'N Stage_N2',
       '6th Stage_IIA', '6th Stage_IIB', '6th Stage_IIIA',
       '6th Stage_IIIB', '6th Stage_IIIC',
      'Grade_1', 'Grade_2', 'Grade_3',
       'Grade_4', 'A Stage_Distant', 'Estrogen Status_Negative',
       'Progesterone Status_Negative']

"""### LR Downsampled"""

from imblearn.under_sampling import RandomUnderSampler
rus = RandomUnderSampler(random_state=40)
X_res, y_res = rus.fit_resample(X_try, y_try)

X_train, X_test, y_train, y_test = train_test_split(X_res, y_res, test_size=0.20,random_state = 20)

#RFE
from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression(max_iter = 1000)
rfe = RFE(logreg, step = 30)
rfe = rfe.fit(X_res[col], y_res.values.ravel())
features = list(X_res[col].columns[rfe.support_])
print(rfe.support_)
print(rfe.ranking_)
print(features)

import statsmodels.api as sm
logit_model=sm.Logit(y_res, X_res[features])
# result=logit_model.fit_regularized()
result = logit_model.fit()
print(result.summary2())

select = ['Race_Black', 'N Stage_N1', '6th Stage_IIIC', 'Grade_1', 'Grade_3', 'Progesterone Status_Negative']

import statsmodels.api as sm
logit_model=sm.Logit(y_res, X_res[select])
# result=logit_model.fit_regularized()
result_select = logit_model.fit()
print(result.summary2())

lr = LogisticRegression(max_iter=10000)

lr.fit(X_train[select], y_train)

predictions = lr.predict(X_test[select])
u_predictions = lr.predict(X_ultimate[select])

from sklearn import metrics
metrics.RocCurveDisplay.from_predictions(y_ultimate,u_predictions)
plt.title('Logistic Regression Downsample')
from sklearn.metrics import classification_report
print('\n Resample Classification Report:\n', classification_report(y_ultimate,u_predictions))
print(metrics.f1_score(y_ultimate,u_predictions))

import numpy as np

params = np.exp(result_select.params)
conf = np.exp(result_select.conf_int())
conf['OR'] = params
conf.columns = ['2.5%','97.5%','OR']
conf

"""### LR Baseline"""

X_train1, X_test1, y_train1, y_test1 = train_test_split(X_try, y_try, test_size=0.20, random_state = 20)

from sklearn.feature_selection import RFE
from sklearn.linear_model import LogisticRegression
logreg = LogisticRegression(max_iter = 1000)
rfe = RFE(logreg, step = 30)
rfe = rfe.fit(X_try[col], y_try.values.ravel())
features1 = list(X_try[col].columns[rfe.support_])
print(rfe.support_)
print(rfe.ranking_)
print(features1)

import statsmodels.api as sm
logit_model=sm.Logit(y_try, X_try[features1])
# result=logit_model.fit_regularized()
result = logit_model.fit()
print(result.summary2())

select1 = ['Race_Black', 'Race_Other', 'Marital Status_Other', 'T Stage_T1',  'T Stage_T4', 'N Stage_N1', 'N Stage_N2', '6th Stage_IIIC', 'Grade_1', 'Grade_2', 'Grade_4', 'Estrogen Status_Negative', 'Progesterone Status_Negative']

import statsmodels.api as sm
logit_model=sm.Logit(y_try, X_try[select1])
# result=logit_model.fit_regularized()
result = logit_model.fit()
print(result.summary2())

lr = LogisticRegression(max_iter=10000)

lr.fit(X_train1[select1], y_train1)

predictions1 = lr.predict(X_test1[select1])
u_predictions1 = lr.predict(X_ultimate[select1])

from sklearn import metrics
metrics.RocCurveDisplay.from_predictions(y_ultimate,u_predictions1)
plt.title('Logistic Regression Baseline')
from sklearn.metrics import classification_report
print('\n Resample Classification Report:\n', classification_report(y_ultimate,u_predictions1))

"""## KNN

### KNN Downsampled
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import StandardScaler

auc_scores = []
scaler = StandardScaler()
scaled_X_train = scaler.fit_transform(X_train)
scaled_X_test = scaler.transform(X_test)

for k in range(1,30):
    knn_model = KNeighborsClassifier(n_neighbors=k)
    knn_model.fit(scaled_X_train,y_train) 
   
    y_pred_test = knn_model.predict(scaled_X_test)
    score = knn_model.predict_proba(scaled_X_test)
    score = score[:,1]
    fpr, tpr, thresholds = metrics.roc_curve(y_test, score)
    auc = metrics.auc(fpr,tpr)
    auc_scores.append(auc)

auc_scores.index(max(auc_scores))

plt.figure(figsize=(8,4),dpi=100)
plt.plot(range(1,30),auc_scores,label='AUC')
plt.legend()
plt.ylabel('AUC')
plt.xlabel("K Value")

knn_final = KNeighborsClassifier(n_neighbors=23)

scaled_X_res = scaler.fit_transform(X_res)
knn_final.fit(scaled_X_res,y_res)

scaled_X_ultimate = scaler.transform(X_ultimate)
y_ultimate_pred = knn_final.predict(scaled_X_ultimate)
metrics.RocCurveDisplay.from_predictions(y_ultimate,y_ultimate_pred)
plt.title('KNN Downsampled (K=23)')

metrics.f1_score(y_ultimate,y_ultimate_pred)

"""### KNN Baseline"""

X_base_train, X_base_test, y_base_train, y_base_test = train_test_split(X_try,y_try, test_size = 0.2, random_state = 20)

auc_scores = []
scaler = StandardScaler()
scaled_X_base_train = scaler.fit_transform(X_base_train)
scaled_X_base_test = scaler.transform(X_base_test)

for k in range(1,30):
    knn_model = KNeighborsClassifier(n_neighbors=k)
    knn_model.fit(scaled_X_base_train,y_base_train) 
   
    y_pred_test = knn_model.predict(scaled_X_base_test)
    score = knn_model.predict_proba(scaled_X_base_test)
    score = score[:,1]
    fpr, tpr, thresholds = metrics.roc_curve(y_base_test, score)
    auc = metrics.auc(fpr,tpr)
    auc_scores.append(auc)

auc_scores.index(max(auc_scores))

scaled_X_try = scaler.transform(X_try)
knn_model = KNeighborsClassifier(n_neighbors=20)
knn_model.fit(scaled_X_try,y_try)

y_ultimate_pred = knn_model.predict(scaled_X_ultimate)
metrics.RocCurveDisplay.from_predictions(y_ultimate,y_ultimate_pred)
plt.title('KNN Baseline (K=20)')

metrics.f1_score(y_ultimate,y_ultimate_pred)

"""## AdaBoost

### AdaBoost Downsampled
"""

from sklearn.ensemble import AdaBoostClassifier

auc_scores = []
for n in range(1,30):
    ada_forest = AdaBoostClassifier(n_estimators=n)
    ada_forest.fit(X_train,y_train)
   
    y_pred_test = ada_forest.predict(X_test)
    score = ada_forest.predict_proba(X_test)
    score = score[:,1]
    fpr, tpr, thresholds = metrics.roc_curve(y_test, score)
    auc = metrics.auc(fpr,tpr)
    auc_scores.append(auc)

auc_scores.index(max(auc_scores))

plt.figure(figsize=(8,4),dpi=100)
plt.plot(range(1,30),auc_scores,label='AUC')
plt.legend()
plt.ylabel('AUC')
plt.xlabel("N Estimators")

final_ada_forest = AdaBoostClassifier(n_estimators=23)
final_ada_forest.fit(X_res,y_res)

y_ultimate_pred = final_ada_forest.predict(X_ultimate)
metrics.RocCurveDisplay.from_predictions(y_ultimate,y_ultimate_pred)
plt.title("AdaBoost Downsampled (n_estimator = 23)")

metrics.f1_score(y_ultimate,y_ultimate_pred)

"""#### Influential Variables"""

final_ada_forest.feature_importances_

feats = pd.DataFrame(index=X_ultimate.columns,data=final_ada_forest.feature_importances_,columns=['Importance'])
imp_feats = feats[feats['Importance']>0]
imp_feats.sort_values('Importance', ascending=False)

plt.figure(figsize=(10,3))
sns.barplot(data=imp_feats.sort_values('Importance'),x=imp_feats.sort_values('Importance').index,y='Importance')
plt.xticks(rotation=90)
plt.title("Feature Importance Plot from Downsampled AdaBoost")

"""### AdaBoost Baseline"""

auc_scores = []
for n in range(1,30):
    ada_forest = AdaBoostClassifier(n_estimators=n)
    ada_forest.fit(X_base_train,y_base_train)
   
    y_pred_test = ada_forest.predict(X_base_test)
    score = ada_forest.predict_proba(X_base_test)
    score = score[:,1]
    fpr, tpr, thresholds = metrics.roc_curve(y_base_test, score)
    auc = metrics.auc(fpr,tpr)
    auc_scores.append(auc)

auc_scores.index(max(auc_scores))

final_ada_forest = AdaBoostClassifier(n_estimators=10)
final_ada_forest.fit(X_try,y_try)

y_ultimate_pred = final_ada_forest.predict(X_ultimate)
metrics.RocCurveDisplay.from_predictions(y_ultimate,y_ultimate_pred)
plt.title("AdaBoost Baseline (n_estimator = 10)")

metrics.f1_score(y_ultimate,y_ultimate_pred)